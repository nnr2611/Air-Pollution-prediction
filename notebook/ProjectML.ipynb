{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fM28u7Scopn",
        "outputId": "ddd4d22d-66a8-43be-b0ef-da99afba0a58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/sid321axn/beijing-multisite-airquality-data-set/versions/1\n",
            "Combined Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 420768 entries, 0 to 420767\n",
            "Data columns (total 19 columns):\n",
            " #   Column   Non-Null Count   Dtype  \n",
            "---  ------   --------------   -----  \n",
            " 0   No       420768 non-null  int64  \n",
            " 1   year     420768 non-null  int64  \n",
            " 2   month    420768 non-null  int64  \n",
            " 3   day      420768 non-null  int64  \n",
            " 4   hour     420768 non-null  int64  \n",
            " 5   PM2.5    412029 non-null  float64\n",
            " 6   PM10     414319 non-null  float64\n",
            " 7   SO2      411747 non-null  float64\n",
            " 8   NO2      408652 non-null  float64\n",
            " 9   CO       400067 non-null  float64\n",
            " 10  O3       407491 non-null  float64\n",
            " 11  TEMP     420370 non-null  float64\n",
            " 12  PRES     420375 non-null  float64\n",
            " 13  DEWP     420365 non-null  float64\n",
            " 14  RAIN     420378 non-null  float64\n",
            " 15  wd       418946 non-null  object \n",
            " 16  WSPM     420450 non-null  float64\n",
            " 17  station  420768 non-null  object \n",
            " 18  site     420768 non-null  object \n",
            "dtypes: float64(11), int64(5), object(3)\n",
            "memory usage: 61.0+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import kagglehub\n",
        "\n",
        "# Download the dataset\n",
        "path = kagglehub.dataset_download(\"sid321axn/beijing-multisite-airquality-data-set\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# File names to be processed\n",
        "list_filenames = [\n",
        "    \"PRSA_Data_Guanyuan_20130301-20170228.csv\",\n",
        "    \"PRSA_Data_Aotizhongxin_20130301-20170228.csv\",\n",
        "    \"PRSA_Data_Wanliu_20130301-20170228.csv\",\n",
        "    \"PRSA_Data_Tiantan_20130301-20170228.csv\",\n",
        "    \"PRSA_Data_Wanshouxigong_20130301-20170228.csv\",\n",
        "    \"PRSA_Data_Nongzhanguan_20130301-20170228.csv\",\n",
        "    \"PRSA_Data_Shunyi_20130301-20170228.csv\",\n",
        "    \"PRSA_Data_Changping_20130301-20170228.csv\",\n",
        "    \"PRSA_Data_Dingling_20130301-20170228.csv\",\n",
        "    \"PRSA_Data_Huairou_20130301-20170228.csv\",\n",
        "    \"PRSA_Data_Gucheng_20130301-20170228.csv\",\n",
        "    \"PRSA_Data_Dongsi_20130301-20170228.csv\"\n",
        "]\n",
        "\n",
        "# Load datasets into a list of dataframes\n",
        "dataframes = []\n",
        "for filename in list_filenames:\n",
        "    full_path = f\"{path}/{filename}\"\n",
        "    df = pd.read_csv(full_path)\n",
        "    df['site'] = filename.split('_')[2]  # Add site identifier\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Combine all dataframes into one\n",
        "combined_data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Display combined data structure\n",
        "print(\"Combined Dataset Info:\")\n",
        "print(combined_data.info())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a datetime column\n",
        "combined_data['datetime'] = pd.to_datetime(combined_data[['year', 'month', 'day', 'hour']])\n",
        "\n",
        "# Sort the dataset by datetime\n",
        "combined_data = combined_data.sort_values(by='datetime')\n",
        "\n",
        "# Verify the update\n",
        "print(\"Data sorted by datetime successfully.\")\n",
        "print(combined_data[['datetime', 'year', 'month', 'day', 'hour']].head())\n",
        "\n",
        "# Sort data by time for temporal consistency\n",
        "combined_data = combined_data.sort_values(by='datetime')\n",
        "\n",
        "# Inspect missing values\n",
        "missing_summary = combined_data.isnull().sum()\n",
        "\n",
        "# Drop columns with too many missing values or irrelevant ones\n",
        "threshold = 0.5 * len(combined_data)  # Drop columns with >50% missing\n",
        "combined_data_cleaned = combined_data.dropna(axis=1, thresh=threshold)\n",
        "\n",
        "# Fill remaining missing values with forward fill or a constant\n",
        "combined_data_cleaned = combined_data_cleaned.fillna(method='ffill').fillna(0)\n",
        "\n",
        "# Feature Engineering\n",
        "# Add temporal features\n",
        "combined_data_cleaned['hour'] = combined_data_cleaned['datetime'].dt.hour\n",
        "combined_data_cleaned['day'] = combined_data_cleaned['datetime'].dt.day\n",
        "combined_data_cleaned['month'] = combined_data_cleaned['datetime'].dt.month\n",
        "combined_data_cleaned['year'] = combined_data_cleaned['datetime'].dt.year\n",
        "combined_data_cleaned['day_of_week'] = combined_data_cleaned['datetime'].dt.dayofweek\n",
        "combined_data_cleaned['is_weekend'] = combined_data_cleaned['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Add a target variable for 7-day ahead prediction\n",
        "combined_data_cleaned['target_7_days_ahead'] = combined_data_cleaned['PM2.5'].shift(-7)  # Shift PM2.5 by 7 days\n",
        "\n",
        "# Add spatial granularity\n",
        "combined_data_cleaned['city_id'] = combined_data_cleaned['site'].astype('category').cat.codes\n",
        "\n",
        "# Generate lag features for AQI or other pollutants\n",
        "lag_features = ['PM2.5', 'PM10', 'O3']\n",
        "for feature in lag_features:\n",
        "    for lag in range(1, 8):  # Create lag features for up to 7 days\n",
        "        combined_data_cleaned[f'{feature}_lag_{lag}'] = combined_data_cleaned[feature].shift(lag)\n",
        "\n",
        "# Generate rolling statistics\n",
        "for feature in lag_features:\n",
        "    combined_data_cleaned[f'{feature}_rolling_mean'] = combined_data_cleaned[feature].rolling(window=7).mean()\n",
        "    combined_data_cleaned[f'{feature}_rolling_std'] = combined_data_cleaned[feature].rolling(window=7).std()\n",
        "\n",
        "# Drop rows with NaN values introduced by rolling or lagging\n",
        "combined_data_cleaned = combined_data_cleaned.dropna()\n",
        "\n",
        "# Save cleaned and feature-engineered data to a CSV for further use\n",
        "combined_data_cleaned.to_csv('processed_beijing_airquality_data.csv', index=False)\n",
        "\n",
        "# Output dataset structure and head\n",
        "print(\"Processed Dataset Info:\")\n",
        "print(combined_data_cleaned.info())\n",
        "print(combined_data_cleaned.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HY56ccUzuIq",
        "outputId": "b94f8c9b-ed0a-45d8-a9bb-b81660c197fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data sorted by datetime successfully.\n",
            "         datetime  year  month  day  hour\n",
            "0      2013-03-01  2013      3    1     0\n",
            "315576 2013-03-01  2013      3    1     0\n",
            "70128  2013-03-01  2013      3    1     0\n",
            "350640 2013-03-01  2013      3    1     0\n",
            "35064  2013-03-01  2013      3    1     0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-f1ecf49959b3>:25: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  combined_data_cleaned = combined_data_cleaned.fillna(method='ffill').fillna(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 420754 entries, 35064 to 175319\n",
            "Data columns (total 51 columns):\n",
            " #   Column               Non-Null Count   Dtype         \n",
            "---  ------               --------------   -----         \n",
            " 0   No                   420754 non-null  int64         \n",
            " 1   year                 420754 non-null  int32         \n",
            " 2   month                420754 non-null  int32         \n",
            " 3   day                  420754 non-null  int32         \n",
            " 4   hour                 420754 non-null  int32         \n",
            " 5   PM2.5                420754 non-null  float64       \n",
            " 6   PM10                 420754 non-null  float64       \n",
            " 7   SO2                  420754 non-null  float64       \n",
            " 8   NO2                  420754 non-null  float64       \n",
            " 9   CO                   420754 non-null  float64       \n",
            " 10  O3                   420754 non-null  float64       \n",
            " 11  TEMP                 420754 non-null  float64       \n",
            " 12  PRES                 420754 non-null  float64       \n",
            " 13  DEWP                 420754 non-null  float64       \n",
            " 14  RAIN                 420754 non-null  float64       \n",
            " 15  wd                   420754 non-null  object        \n",
            " 16  WSPM                 420754 non-null  float64       \n",
            " 17  station              420754 non-null  object        \n",
            " 18  site                 420754 non-null  object        \n",
            " 19  datetime             420754 non-null  datetime64[ns]\n",
            " 20  day_of_week          420754 non-null  int32         \n",
            " 21  is_weekend           420754 non-null  int64         \n",
            " 22  target_7_days_ahead  420754 non-null  float64       \n",
            " 23  city_id              420754 non-null  int8          \n",
            " 24  PM2.5_lag_1          420754 non-null  float64       \n",
            " 25  PM2.5_lag_2          420754 non-null  float64       \n",
            " 26  PM2.5_lag_3          420754 non-null  float64       \n",
            " 27  PM2.5_lag_4          420754 non-null  float64       \n",
            " 28  PM2.5_lag_5          420754 non-null  float64       \n",
            " 29  PM2.5_lag_6          420754 non-null  float64       \n",
            " 30  PM2.5_lag_7          420754 non-null  float64       \n",
            " 31  PM10_lag_1           420754 non-null  float64       \n",
            " 32  PM10_lag_2           420754 non-null  float64       \n",
            " 33  PM10_lag_3           420754 non-null  float64       \n",
            " 34  PM10_lag_4           420754 non-null  float64       \n",
            " 35  PM10_lag_5           420754 non-null  float64       \n",
            " 36  PM10_lag_6           420754 non-null  float64       \n",
            " 37  PM10_lag_7           420754 non-null  float64       \n",
            " 38  O3_lag_1             420754 non-null  float64       \n",
            " 39  O3_lag_2             420754 non-null  float64       \n",
            " 40  O3_lag_3             420754 non-null  float64       \n",
            " 41  O3_lag_4             420754 non-null  float64       \n",
            " 42  O3_lag_5             420754 non-null  float64       \n",
            " 43  O3_lag_6             420754 non-null  float64       \n",
            " 44  O3_lag_7             420754 non-null  float64       \n",
            " 45  PM2.5_rolling_mean   420754 non-null  float64       \n",
            " 46  PM2.5_rolling_std    420754 non-null  float64       \n",
            " 47  PM10_rolling_mean    420754 non-null  float64       \n",
            " 48  PM10_rolling_std     420754 non-null  float64       \n",
            " 49  O3_rolling_mean      420754 non-null  float64       \n",
            " 50  O3_rolling_std       420754 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(39), int32(5), int64(2), int8(1), object(3)\n",
            "memory usage: 156.1+ MB\n",
            "None\n",
            "        No  year  month  day  hour  PM2.5  PM10  SO2   NO2     CO  ...  \\\n",
            "35064    1  2013      3    1     0    4.0   4.0  4.0   7.0  300.0  ...   \n",
            "350640   1  2013      3    1     0    6.0  18.0  5.0   7.0  800.0  ...   \n",
            "70128    1  2013      3    1     0    8.0   8.0  6.0  28.0  400.0  ...   \n",
            "315576   1  2013      3    1     0    7.0   7.0  3.0   2.0  100.0  ...   \n",
            "280512   1  2013      3    1     0    4.0   4.0  3.0   2.0  200.0  ...   \n",
            "\n",
            "        O3_lag_4  O3_lag_5  O3_lag_6  O3_lag_7  PM2.5_rolling_mean  \\\n",
            "35064       85.0      62.0      81.0      69.0            5.571429   \n",
            "350640      89.0      85.0      62.0      81.0            5.571429   \n",
            "70128       85.0      89.0      85.0      62.0            5.428571   \n",
            "315576      44.0      85.0      89.0      85.0            5.714286   \n",
            "280512      77.0      44.0      85.0      89.0            5.000000   \n",
            "\n",
            "       PM2.5_rolling_std  PM10_rolling_mean PM10_rolling_std O3_rolling_mean  \\\n",
            "35064           2.572751           7.714286         3.302236       74.714286   \n",
            "350640          2.572751           9.428571         4.961759       75.714286   \n",
            "70128           2.370453           9.285714         4.990467       74.285714   \n",
            "315576          2.429972           8.285714         4.572173       75.142857   \n",
            "280512          2.000000           7.571429         4.825527       74.142857   \n",
            "\n",
            "       O3_rolling_std  \n",
            "35064       16.152547  \n",
            "350640      16.809861  \n",
            "70128       18.508685  \n",
            "315576      19.213091  \n",
            "280512      18.542101  \n",
            "\n",
            "[5 rows x 51 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras_tuner as kt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_columns = combined_data_cleaned.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Step 1: Prepare the data for LSTM with new features\n",
        "def create_sequences(data, sequence_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        X.append(data[i:i+sequence_length, :-1])  # features\n",
        "        y.append(data[i+sequence_length, -1])    # target variable (PM2.5)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "sequence_length = 7  # Use the last 7 days to predict the next day\n",
        "X, y = create_sequences(combined_data_cleaned[numeric_columns].values, sequence_length)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# X_train and X_test are 3D arrays (samples, time steps, features) for LSTM, scaling should be applied per feature\n",
        "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])  # Flatten the time steps for scaling\n",
        "X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train_reshaped).reshape(X_train.shape)  # Reshape back to 3D\n",
        "X_test_scaled = scaler.transform(X_test_reshaped).reshape(X_test.shape)  # Reshape back to 3D\n",
        "\n",
        "# Step 2: Build the LSTM Model (for hyperparameter tuning)\n",
        "def build_lstm_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=128, step=32),\n",
        "                   return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))  # Output the prediction (PM2.5 value)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Initialize Keras Tuner for LSTM tuning\n",
        "tuner = kt.Hyperband(build_lstm_model, objective='val_loss', max_epochs=10, factor=3, directory='my_dir', project_name='lstm_tuning')\n",
        "\n",
        "# Perform the tuning\n",
        "tuner.search(X_train_scaled, y_train, epochs=10, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# Best hyperparameters for LSTM\n",
        "best_lstm_hp = tuner.get_best_hyperparameters()[0]\n",
        "print(\"Best LSTM hyperparameters:\", best_lstm_hp.values)\n",
        "\n",
        "# Step 3: Train the Best LSTM Model with Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
        "\n",
        "lstm_model = build_lstm_model(best_lstm_hp)\n",
        "lstm_model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, validation_data=(X_test_scaled, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Step 4: Get LSTM Predictions  to use as features for XGBoost\n",
        "lstm_preds_train = lstm_model.predict(X_train_scaled)\n",
        "lstm_preds_test = lstm_model.predict(X_test_scaled)\n",
        "\n",
        "# Save LSTM predictions for next part (XGBoost)\n",
        "np.save('lstm_preds_train.npy', lstm_preds_train)\n",
        "np.save('lstm_preds_test.npy', lstm_preds_test)\n",
        "\n",
        "# save the model\n",
        "lstm_model.save('best_lstm_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfzgtJO32Vs4",
        "outputId": "1e775496-8cb3-4493-aee5-21abc3363bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4 Complete [00h 05m 48s]\n",
            "val_loss: 15.791834831237793\n",
            "\n",
            "Best val_loss So Far: 15.791834831237793\n",
            "Total elapsed time: 00h 14m 34s\n",
            "Best LSTM hyperparameters: {'units': 128, 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 2, 'tuner/round': 0}\n",
            "Epoch 1/20\n",
            "\u001b[1m10519/10519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 14ms/step - loss: 79.4391 - val_loss: 23.0271\n",
            "Epoch 2/20\n",
            "\u001b[1m10519/10519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 14ms/step - loss: 35.2282 - val_loss: 14.8224\n",
            "Epoch 3/20\n",
            "\u001b[1m10519/10519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 15ms/step - loss: 26.7272 - val_loss: 13.1979\n",
            "Epoch 4/20\n",
            "\u001b[1m10519/10519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 15ms/step - loss: 17.7216 - val_loss: 12.5842\n",
            "Epoch 5/20\n",
            "\u001b[1m10519/10519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 14ms/step - loss: 15.4993 - val_loss: 13.2791\n",
            "Epoch 6/20\n",
            "\u001b[1m10519/10519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 14ms/step - loss: 13.1233 - val_loss: 11.9291\n",
            "Epoch 7/20\n",
            "\u001b[1m10519/10519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 14ms/step - loss: 14.4676 - val_loss: 11.6900\n",
            "Epoch 8/20\n",
            "\u001b[1m10519/10519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 14ms/step - loss: 12.9713 - val_loss: 11.5862\n",
            "Epoch 9/20\n",
            "\u001b[1m10519/10519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 14ms/step - loss: 12.1026 - val_loss: 11.8518\n",
            "Epoch 10/20\n",
            "\u001b[1m10519/10519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 14ms/step - loss: 11.3633 - val_loss: 11.8422\n",
            "Epoch 11/20\n",
            "\u001b[1m10519/10519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 14ms/step - loss: 12.4547 - val_loss: 11.7364\n",
            "Epoch 11: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "\u001b[1m10519/10519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 5ms/step\n",
            "\u001b[1m2630/2630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Load the saved LSTM predictions\n",
        "lstm_preds_train = np.load('lstm_preds_train.npy')\n",
        "lstm_preds_test = np.load('lstm_preds_test.npy')\n",
        "\n",
        "\n",
        "# Flatten the predictions to 1D arrays\n",
        "lstm_preds_train = lstm_preds_train.flatten()\n",
        "lstm_preds_test = lstm_preds_test.flatten()\n",
        "\n",
        "# Concatenate the predictions\n",
        "lstm_preds = np.concatenate([lstm_preds_train, lstm_preds_test])\n",
        "\n",
        "# If the lengths don't match, extend lstm_preds with NaN values to match the original data length\n",
        "if len(lstm_preds) < len(combined_data_cleaned):\n",
        "    lstm_preds = np.pad(lstm_preds, (0, len(combined_data_cleaned) - len(lstm_preds)), constant_values=np.nan)\n",
        "\n",
        "# Add LSTM predictions as a feature to the dataset\n",
        "combined_data_cleaned['lstm_preds'] = lstm_preds\n",
        "\n",
        "# Step 5: Prepare Data for XGBoost\n",
        "X = combined_data_cleaned[['PM2.5_lag_1', 'PM10_lag_1', 'O3_lag_1', 'PM2.5_rolling_mean', 'PM10_rolling_mean', 'O3_rolling_mean', 'lstm_preds']]\n",
        "y = combined_data_cleaned['target_7_days_ahead']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize data for XGBoost\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# y_train is sliced to match the size of X_train_scaled\n",
        "y_train = y_train[:X_train_scaled.shape[0]]\n",
        "\n",
        "# Slice X_train_scaled to match the size of lstm_preds_train\n",
        "X_train_scaled = X_train_scaled[:len(lstm_preds_train)]\n",
        "\n",
        "# Combine LSTM predictions with other features for XGBoost (train set)\n",
        "X_train_xgb = np.column_stack([lstm_preds_train[:X_train_scaled.shape[0]], X_train_scaled])\n",
        "\n",
        "# Adjust the length of lstm_preds_test to match X_test_scaled\n",
        "if len(lstm_preds_test) > X_test_scaled.shape[0]:\n",
        "    lstm_preds_test = lstm_preds_test[:X_test_scaled.shape[0]]\n",
        "elif len(lstm_preds_test) < X_test_scaled.shape[0]:\n",
        "    X_test_scaled = X_test_scaled[:len(lstm_preds_test)]\n",
        "\n",
        "# Combine LSTM predictions with other features for XGBoost\n",
        "X_test_xgb = np.column_stack([lstm_preds_test, X_test_scaled])\n",
        "\n",
        "# Step 6: Hyperparameter Tuning for XGBoost using RandomizedSearchCV\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Define a very reduced hyperparameter grid for XGBoost\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100],\n",
        "    'learning_rate': [0.1],\n",
        "    'max_depth': [6],\n",
        "    'subsample': [1.0],\n",
        "}\n",
        "\n",
        "\n",
        "# Align lengths of X_train_xgb and y_train\n",
        "min_length = min(X_train_xgb.shape[0], y_train.shape[0])\n",
        "X_train_xgb = X_train_xgb[:min_length]\n",
        "y_train = y_train[:min_length]\n",
        "\n",
        "# Hyperparameter tuning with RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=xgb_model,\n",
        "                                   param_distributions=param_grid_xgb,\n",
        "                                   n_iter=3,\n",
        "                                   scoring='neg_mean_squared_error',\n",
        "                                   cv=2)\n",
        "random_search.fit(X_train_xgb, y_train)\n",
        "\n",
        "# Output the best hyperparameters\n",
        "print(\"Best parameters for XGBoost:\", random_search.best_params_)\n",
        "\n",
        "# Predict using the best model\n",
        "y_pred_xgb = random_search.best_estimator_.predict(X_test_xgb)\n",
        "\n",
        "# Align lengths\n",
        "if len(y_test) > len(y_pred_xgb):\n",
        "    y_test = y_test[:len(y_pred_xgb)]\n",
        "\n",
        "# Calculate metrics\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"Hybrid Model (LSTM + XGBoost) RMSE: {rmse_xgb}\")\n",
        "print(f\"Hybrid Model (LSTM + XGBoost) MAE: {mae_xgb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxwTibVjLAO2",
        "outputId": "38adfce8-6816-4e44-a243-404b0cc73ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of lstm_preds_train: 336597\n",
            "Length of lstm_preds_test: 84150\n",
            "Length of combined_data_cleaned: 420754\n",
            "Shape of X_train_xgb: (336597, 8)\n",
            "Shape of y_train: (336603,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 1 is smaller than n_iter=3. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1}\n",
            "Hybrid Model (LSTM + XGBoost) RMSE: 30.26101538855598\n",
            "Hybrid Model (LSTM + XGBoost) MAE: 17.394490716805894\n"
          ]
        }
      ]
    }
  ]
}